# Sign Language MNIST with MLP and CNN
This repository contains a Jupyter Notebook (`main.ipynb`) that demonstrates how to build and train both Multilayer Perceptron (MLP) and Convolutional Neural Network (CNN) models for classifying images from the Sign Language MNIST dataset. It's designed to be a helpful resource for those new to computer vision and deep learning, offering a practical comparison between these two common architectures.

![example](https://github.com/user-attachments/assets/f13cb684-b1e1-457d-9692-24238f83c803)

## Overview

This project tackles the task of classifying sign language letters (A-Z,and no cases for 9=J or 25=Z because of gesture motions) represented as 28x28 pixel grayscale images. I use the Sign Language MNIST dataset, which is a great starting point for understanding image classification concepts and it is complex than classic MNIST dataset.    

The primary goal is to:

1.  **Understand the Dataset:** Load and explore the Sign Language MNIST dataset.
2.  **Implement an MLP:** Build a feedforward MLP, a fundamental type of neural network.
3.  **Implement a CNN:** Build a CNN, a powerful architecture for image processing.
4.  **Compare Results:** Train and evaluate both models to see how they perform on the same image classification task and to understand the differences in performance and complexity.
5.  **Provide Clear Code:** Offer a well-commented and easy-to-follow PyTorch implementation.
  
You can find the model [here](https://www.kaggle.com/datasets/datamunge/sign-language-mnist/).
"# Sign-Language-MNIST-with-MLP-and-CNN" 
